<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<title>TORCS: DiscretePolicy Class Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
  $(window).load(resizeHeight);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
  $(document).ready(function() { init_search(); });
</script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="Ticon.png"/></td>
  <td style="padding-left: 0.5em;">
   <div id="projectname">TORCS
   &#160;<span id="projectnumber">1.3.7</span>
   </div>
   <div id="projectbrief">The Open Racing Car Simulator</div>
  </td>
   <td>        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <img id="MSearchSelect" src="search/mag_sel.png"
               onmouseover="return searchBox.OnSearchSelectShow()"
               onmouseout="return searchBox.OnSearchSelectHide()"
               alt=""/>
          <input type="text" id="MSearchField" value="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.png" alt=""/></a>
          </span>
        </div>
</td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.11 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('class_discrete_policy.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="summary">
<a href="#pub-methods">Public Member Functions</a> &#124;
<a href="#pro-methods">Protected Member Functions</a> &#124;
<a href="#pro-attribs">Protected Attributes</a> &#124;
<a href="class_discrete_policy-members.html">List of all members</a>  </div>
  <div class="headertitle">
<div class="title">DiscretePolicy Class Reference</div>  </div>
</div><!--header-->
<div class="contents">

<p>Discrete policies with reinforcement learning.  
 <a href="class_discrete_policy.html#details">More...</a></p>

<p><code>#include &lt;<a class="el" href="policy_8h_source.html">policy.h</a>&gt;</code></p>
<div class="dynheader">
Inheritance diagram for DiscretePolicy:</div>
<div class="dyncontent">
<div class="center"><img src="class_discrete_policy__inherit__graph.png" border="0" usemap="#_discrete_policy_inherit__map" alt="Inheritance graph"/></div>
<map name="_discrete_policy_inherit__map" id="_discrete_policy_inherit__map">
</map>
<center><span class="legend">[<a target="top" href="graph_legend.html">legend</a>]</span></center></div>
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="pub-methods"></a>
Public Member Functions</h2></td></tr>
<tr class="memitem:a38808c7e73befd06d12f631efd61046b"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_discrete_policy.html#a38808c7e73befd06d12f631efd61046b">DiscretePolicy</a> (int <a class="el" href="class_discrete_policy.html#a0299d2d2977c70d5b95e8f5e22564ad9">n_states</a>, int <a class="el" href="class_discrete_policy.html#a4ce76a43e2c48cd148f1de16ffc302ab">n_actions</a>, <a class="el" href="real_8h.html#a031f8951175b43076c2084a6c2173410">real</a> <a class="el" href="class_discrete_policy.html#aa2532317171fce765c11c554811821ba">alpha</a>=0.1, <a class="el" href="real_8h.html#a031f8951175b43076c2084a6c2173410">real</a> <a class="el" href="class_discrete_policy.html#abf219f066c64c027d8340e5f1f864117">gamma</a>=0.8, <a class="el" href="real_8h.html#a031f8951175b43076c2084a6c2173410">real</a> <a class="el" href="class_discrete_policy.html#a937f55bda1533e42c0198965ccb7e7b8">lambda</a>=0.8, bool softmax=false, <a class="el" href="real_8h.html#a031f8951175b43076c2084a6c2173410">real</a> randomness=0.1, <a class="el" href="real_8h.html#a031f8951175b43076c2084a6c2173410">real</a> init_eval=0.0)</td></tr>
<tr class="memdesc:a38808c7e73befd06d12f631efd61046b"><td class="mdescLeft">&#160;</td><td class="mdescRight">Create a new discrete policy.  <a href="#a38808c7e73befd06d12f631efd61046b">More...</a><br /></td></tr>
<tr class="separator:a38808c7e73befd06d12f631efd61046b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:abcbcc865c18db7e34738b0c34e447208"><td class="memItemLeft" align="right" valign="top">virtual&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_discrete_policy.html#abcbcc865c18db7e34738b0c34e447208">~DiscretePolicy</a> ()</td></tr>
<tr class="memdesc:abcbcc865c18db7e34738b0c34e447208"><td class="mdescLeft">&#160;</td><td class="mdescRight">Kill the agent and free everything.  <a href="#abcbcc865c18db7e34738b0c34e447208">More...</a><br /></td></tr>
<tr class="separator:abcbcc865c18db7e34738b0c34e447208"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ad2ce661316086f128b56a43031886f85"><td class="memItemLeft" align="right" valign="top">virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_discrete_policy.html#ad2ce661316086f128b56a43031886f85">setLearningRate</a> (<a class="el" href="real_8h.html#a031f8951175b43076c2084a6c2173410">real</a> <a class="el" href="class_discrete_policy.html#aa2532317171fce765c11c554811821ba">alpha</a>)</td></tr>
<tr class="memdesc:ad2ce661316086f128b56a43031886f85"><td class="mdescLeft">&#160;</td><td class="mdescRight">Set the learning rate.  <a href="#ad2ce661316086f128b56a43031886f85">More...</a><br /></td></tr>
<tr class="separator:ad2ce661316086f128b56a43031886f85"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a95f9342465b2881cc39fbb51427480d0"><td class="memItemLeft" align="right" valign="top">virtual <a class="el" href="real_8h.html#a031f8951175b43076c2084a6c2173410">real</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_discrete_policy.html#a95f9342465b2881cc39fbb51427480d0">getTDError</a> ()</td></tr>
<tr class="memdesc:a95f9342465b2881cc39fbb51427480d0"><td class="mdescLeft">&#160;</td><td class="mdescRight">Get the temporal difference error of the <em>previous</em> action.  <a href="#a95f9342465b2881cc39fbb51427480d0">More...</a><br /></td></tr>
<tr class="separator:a95f9342465b2881cc39fbb51427480d0"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a7c51bd2e37d6906d003e46e5af1f0c54"><td class="memItemLeft" align="right" valign="top">virtual <a class="el" href="real_8h.html#a031f8951175b43076c2084a6c2173410">real</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_discrete_policy.html#a7c51bd2e37d6906d003e46e5af1f0c54">getLastActionValue</a> ()</td></tr>
<tr class="memdesc:a7c51bd2e37d6906d003e46e5af1f0c54"><td class="mdescLeft">&#160;</td><td class="mdescRight">Get the vale of the last action taken.  <a href="#a7c51bd2e37d6906d003e46e5af1f0c54">More...</a><br /></td></tr>
<tr class="separator:a7c51bd2e37d6906d003e46e5af1f0c54"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a5783d913368870603cb8adbeeee30923"><td class="memItemLeft" align="right" valign="top">virtual int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_discrete_policy.html#a5783d913368870603cb8adbeeee30923">SelectAction</a> (int s, <a class="el" href="real_8h.html#a031f8951175b43076c2084a6c2173410">real</a> <a class="el" href="class_discrete_policy.html#aba429d4060d6b1f72cc3ce603bff4f0d">r</a>, int forced_a=-1)</td></tr>
<tr class="memdesc:a5783d913368870603cb8adbeeee30923"><td class="mdescLeft">&#160;</td><td class="mdescRight">Select an action a, given state s and reward from previous action.  <a href="#a5783d913368870603cb8adbeeee30923">More...</a><br /></td></tr>
<tr class="separator:a5783d913368870603cb8adbeeee30923"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a85f150551fe5ffafe5d92edfa4f52099"><td class="memItemLeft" align="right" valign="top">virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_discrete_policy.html#a85f150551fe5ffafe5d92edfa4f52099">Reset</a> ()</td></tr>
<tr class="memdesc:a85f150551fe5ffafe5d92edfa4f52099"><td class="mdescLeft">&#160;</td><td class="mdescRight">Use at the end of every episode, after agent has entered the absorbing state.  <a href="#a85f150551fe5ffafe5d92edfa4f52099">More...</a><br /></td></tr>
<tr class="separator:a85f150551fe5ffafe5d92edfa4f52099"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a6058b276038c93031bf2020652e7fe80"><td class="memItemLeft" align="right" valign="top">virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_discrete_policy.html#a6058b276038c93031bf2020652e7fe80">loadFile</a> (char *f)</td></tr>
<tr class="memdesc:a6058b276038c93031bf2020652e7fe80"><td class="mdescLeft">&#160;</td><td class="mdescRight">Load policy from a file.  <a href="#a6058b276038c93031bf2020652e7fe80">More...</a><br /></td></tr>
<tr class="separator:a6058b276038c93031bf2020652e7fe80"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a44500c0d11b1866f4aff8e291f1ed68b"><td class="memItemLeft" align="right" valign="top">virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_discrete_policy.html#a44500c0d11b1866f4aff8e291f1ed68b">saveFile</a> (char *f)</td></tr>
<tr class="memdesc:a44500c0d11b1866f4aff8e291f1ed68b"><td class="mdescLeft">&#160;</td><td class="mdescRight">Save policy to a file.  <a href="#a44500c0d11b1866f4aff8e291f1ed68b">More...</a><br /></td></tr>
<tr class="separator:a44500c0d11b1866f4aff8e291f1ed68b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a32efaeac5af1bf8b5a74a9ef4644d858"><td class="memItemLeft" align="right" valign="top">virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_discrete_policy.html#a32efaeac5af1bf8b5a74a9ef4644d858">setQLearning</a> ()</td></tr>
<tr class="memdesc:a32efaeac5af1bf8b5a74a9ef4644d858"><td class="mdescLeft">&#160;</td><td class="mdescRight">Set the algorithm to QLearning mode.  <a href="#a32efaeac5af1bf8b5a74a9ef4644d858">More...</a><br /></td></tr>
<tr class="separator:a32efaeac5af1bf8b5a74a9ef4644d858"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae1947bf5cb1ea7ce5d5b8a826406de8a"><td class="memItemLeft" align="right" valign="top">virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_discrete_policy.html#ae1947bf5cb1ea7ce5d5b8a826406de8a">setELearning</a> ()</td></tr>
<tr class="memdesc:ae1947bf5cb1ea7ce5d5b8a826406de8a"><td class="mdescLeft">&#160;</td><td class="mdescRight">Set the algorithm to ELearning mode.  <a href="#ae1947bf5cb1ea7ce5d5b8a826406de8a">More...</a><br /></td></tr>
<tr class="separator:ae1947bf5cb1ea7ce5d5b8a826406de8a"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a99195ad7301204d62186703e489d687f"><td class="memItemLeft" align="right" valign="top">virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_discrete_policy.html#a99195ad7301204d62186703e489d687f">setSarsa</a> ()</td></tr>
<tr class="memdesc:a99195ad7301204d62186703e489d687f"><td class="mdescLeft">&#160;</td><td class="mdescRight">Set the algorithm to SARSA mode.  <a href="#a99195ad7301204d62186703e489d687f">More...</a><br /></td></tr>
<tr class="separator:a99195ad7301204d62186703e489d687f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa9566ce8fc4be68c883e9c067bdb6bc3"><td class="memItemLeft" align="right" valign="top">virtual bool&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_discrete_policy.html#aa9566ce8fc4be68c883e9c067bdb6bc3">useConfidenceEstimates</a> (bool <a class="el" href="class_discrete_policy.html#aa09cc655f3963a9599a4931e94fd1ad5">confidence</a>, <a class="el" href="real_8h.html#a031f8951175b43076c2084a6c2173410">real</a> <a class="el" href="class_discrete_policy.html#a66ff64c205debffe957ca8c904dfcecd">zeta</a>=0.01, bool <a class="el" href="class_discrete_policy.html#ad147be13ff89ba39072a870dd34b2be7">confidence_eligibility</a>=false)</td></tr>
<tr class="memdesc:aa9566ce8fc4be68c883e9c067bdb6bc3"><td class="mdescLeft">&#160;</td><td class="mdescRight">Set to use confidence estimates for action selection, with variance smoothing zeta.  <a href="#aa9566ce8fc4be68c883e9c067bdb6bc3">More...</a><br /></td></tr>
<tr class="separator:aa9566ce8fc4be68c883e9c067bdb6bc3"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a8e68a3cbd81386ac80f1e8ecc04591b2"><td class="memItemLeft" align="right" valign="top">virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_discrete_policy.html#a8e68a3cbd81386ac80f1e8ecc04591b2">setForcedLearning</a> (bool forced)</td></tr>
<tr class="memdesc:a8e68a3cbd81386ac80f1e8ecc04591b2"><td class="mdescLeft">&#160;</td><td class="mdescRight">Set forced learning (force-feed actions)  <a href="#a8e68a3cbd81386ac80f1e8ecc04591b2">More...</a><br /></td></tr>
<tr class="separator:a8e68a3cbd81386ac80f1e8ecc04591b2"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a369c11fbf923820089a6d2cdfd2ef8fe"><td class="memItemLeft" align="right" valign="top">virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_discrete_policy.html#a369c11fbf923820089a6d2cdfd2ef8fe">setRandomness</a> (<a class="el" href="real_8h.html#a031f8951175b43076c2084a6c2173410">real</a> epsilon)</td></tr>
<tr class="memdesc:a369c11fbf923820089a6d2cdfd2ef8fe"><td class="mdescLeft">&#160;</td><td class="mdescRight">Set randomness for action selection. Does not affect confidence mode.  <a href="#a369c11fbf923820089a6d2cdfd2ef8fe">More...</a><br /></td></tr>
<tr class="separator:a369c11fbf923820089a6d2cdfd2ef8fe"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a558dc462679a258d6dd8645af9774eaf"><td class="memItemLeft" align="right" valign="top">virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_discrete_policy.html#a558dc462679a258d6dd8645af9774eaf">setGamma</a> (<a class="el" href="real_8h.html#a031f8951175b43076c2084a6c2173410">real</a> <a class="el" href="class_discrete_policy.html#abf219f066c64c027d8340e5f1f864117">gamma</a>)</td></tr>
<tr class="memdesc:a558dc462679a258d6dd8645af9774eaf"><td class="mdescLeft">&#160;</td><td class="mdescRight">Set the gamma of the sum to be maximised.  <a href="#a558dc462679a258d6dd8645af9774eaf">More...</a><br /></td></tr>
<tr class="separator:a558dc462679a258d6dd8645af9774eaf"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a0d4b562ff61ed37228bdeca1c4d89056"><td class="memItemLeft" align="right" valign="top">virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_discrete_policy.html#a0d4b562ff61ed37228bdeca1c4d89056">setPursuit</a> (bool <a class="el" href="class_discrete_policy.html#aa4b64c80c15b12a77e8aeeb0af677254">pursuit</a>)</td></tr>
<tr class="memdesc:a0d4b562ff61ed37228bdeca1c4d89056"><td class="mdescLeft">&#160;</td><td class="mdescRight">Use Pursuit for action selection.  <a href="#a0d4b562ff61ed37228bdeca1c4d89056">More...</a><br /></td></tr>
<tr class="separator:a0d4b562ff61ed37228bdeca1c4d89056"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a9486c0fd9c2d51954eb048597d681495"><td class="memItemLeft" align="right" valign="top">virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_discrete_policy.html#a9486c0fd9c2d51954eb048597d681495">setReplacingTraces</a> (bool replacing)</td></tr>
<tr class="memdesc:a9486c0fd9c2d51954eb048597d681495"><td class="mdescLeft">&#160;</td><td class="mdescRight">Use Pursuit for action selection.  <a href="#a9486c0fd9c2d51954eb048597d681495">More...</a><br /></td></tr>
<tr class="separator:a9486c0fd9c2d51954eb048597d681495"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a676e1f828240b838e8c76ba9bd9183c0"><td class="memItemLeft" align="right" valign="top">virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_discrete_policy.html#a676e1f828240b838e8c76ba9bd9183c0">useSoftmax</a> (bool softmax)</td></tr>
<tr class="memdesc:a676e1f828240b838e8c76ba9bd9183c0"><td class="mdescLeft">&#160;</td><td class="mdescRight">Set action selection to softmax.  <a href="#a676e1f828240b838e8c76ba9bd9183c0">More...</a><br /></td></tr>
<tr class="separator:a676e1f828240b838e8c76ba9bd9183c0"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a9cf86ac98c427cf3e8433d423c934b38"><td class="memItemLeft" align="right" valign="top">virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_discrete_policy.html#a9cf86ac98c427cf3e8433d423c934b38">setConfidenceDistribution</a> (enum <a class="el" href="policy_8h.html#a9517ab9def7b708d35806750d111ed8a">ConfidenceDistribution</a> cd)</td></tr>
<tr class="memdesc:a9cf86ac98c427cf3e8433d423c934b38"><td class="mdescLeft">&#160;</td><td class="mdescRight">Set the distribution for direct action sampling.  <a href="#a9cf86ac98c427cf3e8433d423c934b38">More...</a><br /></td></tr>
<tr class="separator:a9cf86ac98c427cf3e8433d423c934b38"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aead952ca5598727cbff910796aca9865"><td class="memItemLeft" align="right" valign="top">virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_discrete_policy.html#aead952ca5598727cbff910796aca9865">useGibbsConfidence</a> (bool gibbs)</td></tr>
<tr class="memdesc:aead952ca5598727cbff910796aca9865"><td class="mdescLeft">&#160;</td><td class="mdescRight">Add Gibbs sampling for confidences.  <a href="#aead952ca5598727cbff910796aca9865">More...</a><br /></td></tr>
<tr class="separator:aead952ca5598727cbff910796aca9865"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ad8bc316900503b360bae7a19ce714a12"><td class="memItemLeft" align="right" valign="top">virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_discrete_policy.html#ad8bc316900503b360bae7a19ce714a12">useReliabilityEstimate</a> (bool ri)</td></tr>
<tr class="memdesc:ad8bc316900503b360bae7a19ce714a12"><td class="mdescLeft">&#160;</td><td class="mdescRight">Use the reliability estimate method for action selection.  <a href="#ad8bc316900503b360bae7a19ce714a12">More...</a><br /></td></tr>
<tr class="separator:ad8bc316900503b360bae7a19ce714a12"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a52843d1894e52680a66ed95540620ebe"><td class="memItemLeft" align="right" valign="top">virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_discrete_policy.html#a52843d1894e52680a66ed95540620ebe">saveState</a> (FILE *f)</td></tr>
<tr class="memdesc:a52843d1894e52680a66ed95540620ebe"><td class="mdescLeft">&#160;</td><td class="mdescRight">Save the current evaluations in text format to a file.  <a href="#a52843d1894e52680a66ed95540620ebe">More...</a><br /></td></tr>
<tr class="separator:a52843d1894e52680a66ed95540620ebe"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="pro-methods"></a>
Protected Member Functions</h2></td></tr>
<tr class="memitem:aef97e522d2cd647a0260f0bc77508c05"><td class="memItemLeft" align="right" valign="top">int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_discrete_policy.html#aef97e522d2cd647a0260f0bc77508c05">confMax</a> (<a class="el" href="real_8h.html#a031f8951175b43076c2084a6c2173410">real</a> *Qs, <a class="el" href="real_8h.html#a031f8951175b43076c2084a6c2173410">real</a> *vQs, <a class="el" href="real_8h.html#a031f8951175b43076c2084a6c2173410">real</a> <a class="el" href="_convex_8cpp.html#a4a5bd01d70bbe801840501598c71de1e">p</a>=1.0)</td></tr>
<tr class="memdesc:aef97e522d2cd647a0260f0bc77508c05"><td class="mdescLeft">&#160;</td><td class="mdescRight">Confidence-based Gibbs sampling.  <a href="#aef97e522d2cd647a0260f0bc77508c05">More...</a><br /></td></tr>
<tr class="separator:aef97e522d2cd647a0260f0bc77508c05"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a0d68a28991722e21bf90f7f34fa7e41e"><td class="memItemLeft" align="right" valign="top">int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_discrete_policy.html#a0d68a28991722e21bf90f7f34fa7e41e">confSample</a> (<a class="el" href="real_8h.html#a031f8951175b43076c2084a6c2173410">real</a> *Qs, <a class="el" href="real_8h.html#a031f8951175b43076c2084a6c2173410">real</a> *vQs)</td></tr>
<tr class="memdesc:a0d68a28991722e21bf90f7f34fa7e41e"><td class="mdescLeft">&#160;</td><td class="mdescRight">Directly sample from action value distribution.  <a href="#a0d68a28991722e21bf90f7f34fa7e41e">More...</a><br /></td></tr>
<tr class="separator:a0d68a28991722e21bf90f7f34fa7e41e"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1a82bb1b411851f8baeedd532b4a4bc1"><td class="memItemLeft" align="right" valign="top">int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_discrete_policy.html#a1a82bb1b411851f8baeedd532b4a4bc1">softMax</a> (<a class="el" href="real_8h.html#a031f8951175b43076c2084a6c2173410">real</a> *Qs)</td></tr>
<tr class="memdesc:a1a82bb1b411851f8baeedd532b4a4bc1"><td class="mdescLeft">&#160;</td><td class="mdescRight">Softmax Gibbs sampling.  <a href="#a1a82bb1b411851f8baeedd532b4a4bc1">More...</a><br /></td></tr>
<tr class="separator:a1a82bb1b411851f8baeedd532b4a4bc1"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a969a17dda8b4a094f0f9ea0076aefa5b"><td class="memItemLeft" align="right" valign="top">int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_discrete_policy.html#a969a17dda8b4a094f0f9ea0076aefa5b">eGreedy</a> (<a class="el" href="real_8h.html#a031f8951175b43076c2084a6c2173410">real</a> *Qs)</td></tr>
<tr class="memdesc:a969a17dda8b4a094f0f9ea0076aefa5b"><td class="mdescLeft">&#160;</td><td class="mdescRight">e-greedy sampling  <a href="#a969a17dda8b4a094f0f9ea0076aefa5b">More...</a><br /></td></tr>
<tr class="separator:a969a17dda8b4a094f0f9ea0076aefa5b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a280c85ce976de3150f772ed4c772c1e0"><td class="memItemLeft" align="right" valign="top">int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_discrete_policy.html#a280c85ce976de3150f772ed4c772c1e0">argMax</a> (<a class="el" href="real_8h.html#a031f8951175b43076c2084a6c2173410">real</a> *Qs)</td></tr>
<tr class="memdesc:a280c85ce976de3150f772ed4c772c1e0"><td class="mdescLeft">&#160;</td><td class="mdescRight">Get ID of maximum action.  <a href="#a280c85ce976de3150f772ed4c772c1e0">More...</a><br /></td></tr>
<tr class="separator:a280c85ce976de3150f772ed4c772c1e0"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="pro-attribs"></a>
Protected Attributes</h2></td></tr>
<tr class="memitem:a6a6aef0d063dde7ae1037effe7d3d9e4"><td class="memItemLeft" align="right" valign="top">enum <a class="el" href="policy_8h.html#aff580e7ad2896c3a6b8863811234469a">LearningMethod</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_discrete_policy.html#a6a6aef0d063dde7ae1037effe7d3d9e4">learning_method</a></td></tr>
<tr class="memdesc:a6a6aef0d063dde7ae1037effe7d3d9e4"><td class="mdescLeft">&#160;</td><td class="mdescRight">learning method to use;  <a href="#a6a6aef0d063dde7ae1037effe7d3d9e4">More...</a><br /></td></tr>
<tr class="separator:a6a6aef0d063dde7ae1037effe7d3d9e4"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a0299d2d2977c70d5b95e8f5e22564ad9"><td class="memItemLeft" align="right" valign="top">int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_discrete_policy.html#a0299d2d2977c70d5b95e8f5e22564ad9">n_states</a></td></tr>
<tr class="memdesc:a0299d2d2977c70d5b95e8f5e22564ad9"><td class="mdescLeft">&#160;</td><td class="mdescRight">number of states  <a href="#a0299d2d2977c70d5b95e8f5e22564ad9">More...</a><br /></td></tr>
<tr class="separator:a0299d2d2977c70d5b95e8f5e22564ad9"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a4ce76a43e2c48cd148f1de16ffc302ab"><td class="memItemLeft" align="right" valign="top">int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_discrete_policy.html#a4ce76a43e2c48cd148f1de16ffc302ab">n_actions</a></td></tr>
<tr class="memdesc:a4ce76a43e2c48cd148f1de16ffc302ab"><td class="mdescLeft">&#160;</td><td class="mdescRight">number of actions  <a href="#a4ce76a43e2c48cd148f1de16ffc302ab">More...</a><br /></td></tr>
<tr class="separator:a4ce76a43e2c48cd148f1de16ffc302ab"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae938f9b15ca6dc7ee6b81f17911a3985"><td class="memItemLeft" align="right" valign="top"><a class="el" href="real_8h.html#a031f8951175b43076c2084a6c2173410">real</a> **&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_discrete_policy.html#ae938f9b15ca6dc7ee6b81f17911a3985">Q</a></td></tr>
<tr class="memdesc:ae938f9b15ca6dc7ee6b81f17911a3985"><td class="mdescLeft">&#160;</td><td class="mdescRight">state-action evaluation  <a href="#ae938f9b15ca6dc7ee6b81f17911a3985">More...</a><br /></td></tr>
<tr class="separator:ae938f9b15ca6dc7ee6b81f17911a3985"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a967c043721521692685f722cd19dde0c"><td class="memItemLeft" align="right" valign="top"><a class="el" href="real_8h.html#a031f8951175b43076c2084a6c2173410">real</a> **&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_discrete_policy.html#a967c043721521692685f722cd19dde0c">e</a></td></tr>
<tr class="memdesc:a967c043721521692685f722cd19dde0c"><td class="mdescLeft">&#160;</td><td class="mdescRight">eligibility trace  <a href="#a967c043721521692685f722cd19dde0c">More...</a><br /></td></tr>
<tr class="separator:a967c043721521692685f722cd19dde0c"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af8e4af6ae154997cc62107c2881c743f"><td class="memItemLeft" align="right" valign="top"><a class="el" href="real_8h.html#a031f8951175b43076c2084a6c2173410">real</a> *&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_discrete_policy.html#af8e4af6ae154997cc62107c2881c743f">eval</a></td></tr>
<tr class="memdesc:af8e4af6ae154997cc62107c2881c743f"><td class="mdescLeft">&#160;</td><td class="mdescRight">evaluation of current aciton  <a href="#af8e4af6ae154997cc62107c2881c743f">More...</a><br /></td></tr>
<tr class="separator:af8e4af6ae154997cc62107c2881c743f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a87dafb916b43cbdb7afada5123be0b57"><td class="memItemLeft" align="right" valign="top"><a class="el" href="real_8h.html#a031f8951175b43076c2084a6c2173410">real</a> *&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_discrete_policy.html#a87dafb916b43cbdb7afada5123be0b57">sample</a></td></tr>
<tr class="memdesc:a87dafb916b43cbdb7afada5123be0b57"><td class="mdescLeft">&#160;</td><td class="mdescRight">sampling output  <a href="#a87dafb916b43cbdb7afada5123be0b57">More...</a><br /></td></tr>
<tr class="separator:a87dafb916b43cbdb7afada5123be0b57"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af732e32c30602639a8a413bad23e7a68"><td class="memItemLeft" align="right" valign="top"><a class="el" href="real_8h.html#a031f8951175b43076c2084a6c2173410">real</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_discrete_policy.html#af732e32c30602639a8a413bad23e7a68">pQ</a></td></tr>
<tr class="memdesc:af732e32c30602639a8a413bad23e7a68"><td class="mdescLeft">&#160;</td><td class="mdescRight">previous Q  <a href="#af732e32c30602639a8a413bad23e7a68">More...</a><br /></td></tr>
<tr class="separator:af732e32c30602639a8a413bad23e7a68"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac5f57533c6235ea465790dbc0ca67d9b"><td class="memItemLeft" align="right" valign="top">int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_discrete_policy.html#ac5f57533c6235ea465790dbc0ca67d9b">ps</a></td></tr>
<tr class="memdesc:ac5f57533c6235ea465790dbc0ca67d9b"><td class="mdescLeft">&#160;</td><td class="mdescRight">previous state  <a href="#ac5f57533c6235ea465790dbc0ca67d9b">More...</a><br /></td></tr>
<tr class="separator:ac5f57533c6235ea465790dbc0ca67d9b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af9c9c25ebbc6099a94b7272d97ef7cd1"><td class="memItemLeft" align="right" valign="top">int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_discrete_policy.html#af9c9c25ebbc6099a94b7272d97ef7cd1">pa</a></td></tr>
<tr class="memdesc:af9c9c25ebbc6099a94b7272d97ef7cd1"><td class="mdescLeft">&#160;</td><td class="mdescRight">previous action  <a href="#af9c9c25ebbc6099a94b7272d97ef7cd1">More...</a><br /></td></tr>
<tr class="separator:af9c9c25ebbc6099a94b7272d97ef7cd1"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aba429d4060d6b1f72cc3ce603bff4f0d"><td class="memItemLeft" align="right" valign="top"><a class="el" href="real_8h.html#a031f8951175b43076c2084a6c2173410">real</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_discrete_policy.html#aba429d4060d6b1f72cc3ce603bff4f0d">r</a></td></tr>
<tr class="memdesc:aba429d4060d6b1f72cc3ce603bff4f0d"><td class="mdescLeft">&#160;</td><td class="mdescRight">reward  <a href="#aba429d4060d6b1f72cc3ce603bff4f0d">More...</a><br /></td></tr>
<tr class="separator:aba429d4060d6b1f72cc3ce603bff4f0d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab03ecbc5cc0c50264069f17ce8ad7430"><td class="memItemLeft" align="right" valign="top"><a class="el" href="real_8h.html#a031f8951175b43076c2084a6c2173410">real</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_discrete_policy.html#ab03ecbc5cc0c50264069f17ce8ad7430">temp</a></td></tr>
<tr class="memdesc:ab03ecbc5cc0c50264069f17ce8ad7430"><td class="mdescLeft">&#160;</td><td class="mdescRight">scratch  <a href="#ab03ecbc5cc0c50264069f17ce8ad7430">More...</a><br /></td></tr>
<tr class="separator:ab03ecbc5cc0c50264069f17ce8ad7430"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a2a324879a3aa500791c42eb844c00623"><td class="memItemLeft" align="right" valign="top"><a class="el" href="real_8h.html#a031f8951175b43076c2084a6c2173410">real</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_discrete_policy.html#a2a324879a3aa500791c42eb844c00623">tdError</a></td></tr>
<tr class="memdesc:a2a324879a3aa500791c42eb844c00623"><td class="mdescLeft">&#160;</td><td class="mdescRight">temporal difference error  <a href="#a2a324879a3aa500791c42eb844c00623">More...</a><br /></td></tr>
<tr class="separator:a2a324879a3aa500791c42eb844c00623"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa7dda786a5b43c89a1430f6d08cc04c2"><td class="memItemLeft" align="right" valign="top">bool&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_discrete_policy.html#aa7dda786a5b43c89a1430f6d08cc04c2">smax</a></td></tr>
<tr class="memdesc:aa7dda786a5b43c89a1430f6d08cc04c2"><td class="mdescLeft">&#160;</td><td class="mdescRight">softmax option  <a href="#aa7dda786a5b43c89a1430f6d08cc04c2">More...</a><br /></td></tr>
<tr class="separator:aa7dda786a5b43c89a1430f6d08cc04c2"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa4b64c80c15b12a77e8aeeb0af677254"><td class="memItemLeft" align="right" valign="top">bool&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_discrete_policy.html#aa4b64c80c15b12a77e8aeeb0af677254">pursuit</a></td></tr>
<tr class="memdesc:aa4b64c80c15b12a77e8aeeb0af677254"><td class="mdescLeft">&#160;</td><td class="mdescRight">pursuit option  <a href="#aa4b64c80c15b12a77e8aeeb0af677254">More...</a><br /></td></tr>
<tr class="separator:aa4b64c80c15b12a77e8aeeb0af677254"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a321300d54340917a8c18aa5bdf70e957"><td class="memItemLeft" align="right" valign="top"><a class="el" href="real_8h.html#a031f8951175b43076c2084a6c2173410">real</a> **&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_discrete_policy.html#a321300d54340917a8c18aa5bdf70e957">P</a></td></tr>
<tr class="memdesc:a321300d54340917a8c18aa5bdf70e957"><td class="mdescLeft">&#160;</td><td class="mdescRight">pursuit action probabilities  <a href="#a321300d54340917a8c18aa5bdf70e957">More...</a><br /></td></tr>
<tr class="separator:a321300d54340917a8c18aa5bdf70e957"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:abf219f066c64c027d8340e5f1f864117"><td class="memItemLeft" align="right" valign="top"><a class="el" href="real_8h.html#a031f8951175b43076c2084a6c2173410">real</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_discrete_policy.html#abf219f066c64c027d8340e5f1f864117">gamma</a></td></tr>
<tr class="memdesc:abf219f066c64c027d8340e5f1f864117"><td class="mdescLeft">&#160;</td><td class="mdescRight">Future discount parameter.  <a href="#abf219f066c64c027d8340e5f1f864117">More...</a><br /></td></tr>
<tr class="separator:abf219f066c64c027d8340e5f1f864117"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a937f55bda1533e42c0198965ccb7e7b8"><td class="memItemLeft" align="right" valign="top"><a class="el" href="real_8h.html#a031f8951175b43076c2084a6c2173410">real</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_discrete_policy.html#a937f55bda1533e42c0198965ccb7e7b8">lambda</a></td></tr>
<tr class="memdesc:a937f55bda1533e42c0198965ccb7e7b8"><td class="mdescLeft">&#160;</td><td class="mdescRight">Eligibility trace decay.  <a href="#a937f55bda1533e42c0198965ccb7e7b8">More...</a><br /></td></tr>
<tr class="separator:a937f55bda1533e42c0198965ccb7e7b8"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa2532317171fce765c11c554811821ba"><td class="memItemLeft" align="right" valign="top"><a class="el" href="real_8h.html#a031f8951175b43076c2084a6c2173410">real</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_discrete_policy.html#aa2532317171fce765c11c554811821ba">alpha</a></td></tr>
<tr class="memdesc:aa2532317171fce765c11c554811821ba"><td class="mdescLeft">&#160;</td><td class="mdescRight">learning rate  <a href="#aa2532317171fce765c11c554811821ba">More...</a><br /></td></tr>
<tr class="separator:aa2532317171fce765c11c554811821ba"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a7dcd73587c35dcc0ee4c88a90a4934d5"><td class="memItemLeft" align="right" valign="top"><a class="el" href="real_8h.html#a031f8951175b43076c2084a6c2173410">real</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_discrete_policy.html#a7dcd73587c35dcc0ee4c88a90a4934d5">expected_r</a></td></tr>
<tr class="memdesc:a7dcd73587c35dcc0ee4c88a90a4934d5"><td class="mdescLeft">&#160;</td><td class="mdescRight">Expected reward.  <a href="#a7dcd73587c35dcc0ee4c88a90a4934d5">More...</a><br /></td></tr>
<tr class="separator:a7dcd73587c35dcc0ee4c88a90a4934d5"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac971a0b4531befd7e35b0d213abfc18b"><td class="memItemLeft" align="right" valign="top"><a class="el" href="real_8h.html#a031f8951175b43076c2084a6c2173410">real</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_discrete_policy.html#ac971a0b4531befd7e35b0d213abfc18b">expected_V</a></td></tr>
<tr class="memdesc:ac971a0b4531befd7e35b0d213abfc18b"><td class="mdescLeft">&#160;</td><td class="mdescRight">Expected state return.  <a href="#ac971a0b4531befd7e35b0d213abfc18b">More...</a><br /></td></tr>
<tr class="separator:ac971a0b4531befd7e35b0d213abfc18b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af4e2f051947d822f48ad41e72d9ace60"><td class="memItemLeft" align="right" valign="top">int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_discrete_policy.html#af4e2f051947d822f48ad41e72d9ace60">n_samples</a></td></tr>
<tr class="memdesc:af4e2f051947d822f48ad41e72d9ace60"><td class="mdescLeft">&#160;</td><td class="mdescRight">number of samples for above expected r and V  <a href="#af4e2f051947d822f48ad41e72d9ace60">More...</a><br /></td></tr>
<tr class="separator:af4e2f051947d822f48ad41e72d9ace60"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a91a4b8869983fc306f666c8a53e3c392"><td class="memItemLeft" align="right" valign="top">int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_discrete_policy.html#a91a4b8869983fc306f666c8a53e3c392">min_el_state</a></td></tr>
<tr class="memdesc:a91a4b8869983fc306f666c8a53e3c392"><td class="mdescLeft">&#160;</td><td class="mdescRight">min state ID to search for eligibility  <a href="#a91a4b8869983fc306f666c8a53e3c392">More...</a><br /></td></tr>
<tr class="separator:a91a4b8869983fc306f666c8a53e3c392"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af73d4c7ea1251b74c3b6ee1f112d7308"><td class="memItemLeft" align="right" valign="top">int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_discrete_policy.html#af73d4c7ea1251b74c3b6ee1f112d7308">max_el_state</a></td></tr>
<tr class="memdesc:af73d4c7ea1251b74c3b6ee1f112d7308"><td class="mdescLeft">&#160;</td><td class="mdescRight">max state ID to search for eligibility  <a href="#af73d4c7ea1251b74c3b6ee1f112d7308">More...</a><br /></td></tr>
<tr class="separator:af73d4c7ea1251b74c3b6ee1f112d7308"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a39c6a2209b93281890fdf2979eb80e07"><td class="memItemLeft" align="right" valign="top">bool&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_discrete_policy.html#a39c6a2209b93281890fdf2979eb80e07">replacing_traces</a></td></tr>
<tr class="memdesc:a39c6a2209b93281890fdf2979eb80e07"><td class="mdescLeft">&#160;</td><td class="mdescRight">Replacing instead of accumulating traces.  <a href="#a39c6a2209b93281890fdf2979eb80e07">More...</a><br /></td></tr>
<tr class="separator:a39c6a2209b93281890fdf2979eb80e07"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a8612bd729565c78d029fe321981bc640"><td class="memItemLeft" align="right" valign="top">bool&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_discrete_policy.html#a8612bd729565c78d029fe321981bc640">forced_learning</a></td></tr>
<tr class="memdesc:a8612bd729565c78d029fe321981bc640"><td class="mdescLeft">&#160;</td><td class="mdescRight">Force agent to take supplied action.  <a href="#a8612bd729565c78d029fe321981bc640">More...</a><br /></td></tr>
<tr class="separator:a8612bd729565c78d029fe321981bc640"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa09cc655f3963a9599a4931e94fd1ad5"><td class="memItemLeft" align="right" valign="top">bool&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_discrete_policy.html#aa09cc655f3963a9599a4931e94fd1ad5">confidence</a></td></tr>
<tr class="memdesc:aa09cc655f3963a9599a4931e94fd1ad5"><td class="mdescLeft">&#160;</td><td class="mdescRight">Confidence estimates option.  <a href="#aa09cc655f3963a9599a4931e94fd1ad5">More...</a><br /></td></tr>
<tr class="separator:aa09cc655f3963a9599a4931e94fd1ad5"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ad147be13ff89ba39072a870dd34b2be7"><td class="memItemLeft" align="right" valign="top">bool&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_discrete_policy.html#ad147be13ff89ba39072a870dd34b2be7">confidence_eligibility</a></td></tr>
<tr class="memdesc:ad147be13ff89ba39072a870dd34b2be7"><td class="mdescLeft">&#160;</td><td class="mdescRight">Apply eligibility traces to confidence.  <a href="#ad147be13ff89ba39072a870dd34b2be7">More...</a><br /></td></tr>
<tr class="separator:ad147be13ff89ba39072a870dd34b2be7"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a9202989ffb7177a4c9145a01191abbae"><td class="memItemLeft" align="right" valign="top">bool&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_discrete_policy.html#a9202989ffb7177a4c9145a01191abbae">reliability_estimate</a></td></tr>
<tr class="memdesc:a9202989ffb7177a4c9145a01191abbae"><td class="mdescLeft">&#160;</td><td class="mdescRight">reliability estimates option  <a href="#a9202989ffb7177a4c9145a01191abbae">More...</a><br /></td></tr>
<tr class="separator:a9202989ffb7177a4c9145a01191abbae"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ad2d84a7ca4c2a8c8a807a11f73041df7"><td class="memItemLeft" align="right" valign="top">enum <a class="el" href="policy_8h.html#a9517ab9def7b708d35806750d111ed8a">ConfidenceDistribution</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_discrete_policy.html#ad2d84a7ca4c2a8c8a807a11f73041df7">confidence_distribution</a></td></tr>
<tr class="memdesc:ad2d84a7ca4c2a8c8a807a11f73041df7"><td class="mdescLeft">&#160;</td><td class="mdescRight"><a class="el" href="class_distribution.html" title="Probability distribution. ">Distribution</a> to use for confidence sampling.  <a href="#ad2d84a7ca4c2a8c8a807a11f73041df7">More...</a><br /></td></tr>
<tr class="separator:ad2d84a7ca4c2a8c8a807a11f73041df7"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab4674e239def6c36c8b814ca933e2c95"><td class="memItemLeft" align="right" valign="top">bool&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_discrete_policy.html#ab4674e239def6c36c8b814ca933e2c95">confidence_uses_gibbs</a></td></tr>
<tr class="memdesc:ab4674e239def6c36c8b814ca933e2c95"><td class="mdescLeft">&#160;</td><td class="mdescRight">Additional gibbs sampling for confidence.  <a href="#ab4674e239def6c36c8b814ca933e2c95">More...</a><br /></td></tr>
<tr class="separator:ab4674e239def6c36c8b814ca933e2c95"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a66ff64c205debffe957ca8c904dfcecd"><td class="memItemLeft" align="right" valign="top"><a class="el" href="real_8h.html#a031f8951175b43076c2084a6c2173410">real</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_discrete_policy.html#a66ff64c205debffe957ca8c904dfcecd">zeta</a></td></tr>
<tr class="memdesc:a66ff64c205debffe957ca8c904dfcecd"><td class="mdescLeft">&#160;</td><td class="mdescRight">Confidence smoothing.  <a href="#a66ff64c205debffe957ca8c904dfcecd">More...</a><br /></td></tr>
<tr class="separator:a66ff64c205debffe957ca8c904dfcecd"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a0fc9cc4ca7542cfbce9363b62d243ff6"><td class="memItemLeft" align="right" valign="top"><a class="el" href="real_8h.html#a031f8951175b43076c2084a6c2173410">real</a> **&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_discrete_policy.html#a0fc9cc4ca7542cfbce9363b62d243ff6">vQ</a></td></tr>
<tr class="memdesc:a0fc9cc4ca7542cfbce9363b62d243ff6"><td class="mdescLeft">&#160;</td><td class="mdescRight">variance estimate for Q  <a href="#a0fc9cc4ca7542cfbce9363b62d243ff6">More...</a><br /></td></tr>
<tr class="separator:a0fc9cc4ca7542cfbce9363b62d243ff6"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><p>Discrete policies with reinforcement learning. </p>
<p>This class implements a discrete policy using the Sarsa( <img class="formulaInl" alt="$\lambda$" src="form_24.png"/>) or Q( <img class="formulaInl" alt="$\lambda$" src="form_24.png"/>) Reinforcement Learning algorithm. After creating an instance of the algorithm were the number of actions and states are specified, you can call the <a class="el" href="class_discrete_policy.html#a5783d913368870603cb8adbeeee30923" title="Select an action a, given state s and reward from previous action. ">SelectAction()</a> method at every time step to dynamically select an action according to the given state. At the same time you must provide a reinforcement, which should be large (or positive) for when the algorithm is doing well, and small (or negative) when it is not. The algorithm will try and maximise the total reward received.</p>
<p>Parameters:</p>
<p>n_states: The number of discrete states, or situations, that the agent is in. You should create states that are relevant to the task.</p>
<p>n_actions: The number of different things the agent can do at every state. Currently we assume that all actions are usable at all states. However if an action a_i is not usable in some state s, then you can make the client side of the code for state s map state a_i to some usable state a_j, meaning that when the agent selects action a_i in state s, the result would be that of taking one of the usable actions. This will make the two actions equivalent. Alternatively when the agent selects an unusable action at a particular state, you could have a random outcome of actions. The algorithm can take care of that, since it assumes no relationship between the same action at different states.</p>
<p>alpha: Learning rate. Controls how fast the evaluation is changed at every time step. Good values are between 0.01 and 0.1. Lower than 0.01 makes learning too slow, higher than 0.1 makes learning unstable, particularly for high gamma and lambda.</p>
<p>gamma: The algorithm will maximise the exponentially decaying sum of all future rewards, were the base of the exponent is gamma. If gamma=0, then the algorithm will always favour short-term rewards over long-term ones. Setting gamma close to 1 will make long-term rewards more important.</p>
<p>lambda: This controls how much the expected value of reinforcement is close to the observed value of reinforcement. Another view is that of it controlling the eligibility traces e. Eligibility traces perform temporal credit assignment to previous actions/states. High values of lambda (near 1) can speed up learning. With lambda=0, only the currently selected action/state pair evaluation is updated. With lambda&gt;0 all state/action pair evaluations taken in the past are updated, with most recent pairs updated more than pairs further in the past. Ultimately the optimal value of lambda will depend on the task.</p>
<p>softmax, randomness: If this is false, then the algorithm selects the best possible action all the time, but with probability 'randomness' selects a completely random action at each timestep. If this is true, then the algorithm selects actions stochastically, with probability of selecting each action proportional to how better it seems to be than the others. A high randomness (&gt;10.0) will create more or less equal probabilities for actions, while a low randomness (&lt;0.1) will make the system almost deterministic.</p>
<p>init_eval: This is the initial evaluation for actions. It pays to set this to optimistic values (i.e. higher than the reinforcement to be given), so that the algorithm becomes always 'disappointed' and tries to explore as much of the possible combinations as it can.</p>
<p>Member functions:</p>
<p><a class="el" href="class_discrete_policy.html#a32efaeac5af1bf8b5a74a9ef4644d858" title="Set the algorithm to QLearning mode. ">setQLearning()</a>: Use the Q( <img class="formulaInl" alt="$\lambda$" src="form_24.png"/>) algorithm</p>
<p><a class="el" href="class_discrete_policy.html#ae1947bf5cb1ea7ce5d5b8a826406de8a" title="Set the algorithm to ELearning mode. ">setELearning()</a>: Use the E( <img class="formulaInl" alt="$\lambda$" src="form_24.png"/>) algorithm</p>
<p><a class="el" href="class_discrete_policy.html#a99195ad7301204d62186703e489d687f" title="Set the algorithm to SARSA mode. ">setSarsa()</a>: Use the Sarsa( <img class="formulaInl" alt="$\lambda$" src="form_24.png"/>) algorithm</p>
<p>All the above algorithms attempt to approximate the optimal value function by using an update of the form </p><p class="formulaDsp">
<img class="formulaDsp" alt="\[ Q_{t+1}(s,a) = Q_{t}(s,a) + \alpha (r_{t+1} + \gamma E\{Q(s'|\pi)\} - Q_{t}(s,a)). \]" src="form_47.png"/>
</p>
<p>The difference lies in how the expectation operator is approximated. In the case of SARSA, it is done by directly sampling the current policy, i.e. the expected value of Q for the next state is the current evaluation for the action actually taken in the next state. In Q-learning the expected value is that of the greedy action in the next state (with some minor complications to accommodate eligibility traces). E-learning is the most general case. The simplest implementation works by just replacing the single sample from Q with <img class="formulaInl" alt="$\sum_{b} Q(s',b) P(a'=b|s')$" src="form_48.png"/>. This lies somewhere in between SARSA and Q-learning.</p>
<p>setPursuit (bool pursuit): If true, use pursuit methods to determine the best possible action. This enforces maximum exploration initially, and maximum exploitation of estimates later. I am not sure of the convergence properties of this method, however, when used in conjuction with Sarsa or Q-learning.</p>
<p>useConfidenceEstimates(bool confidence, float zeta): Use confidence estimates for the estimated parameters. This allows automatic exploration-exploitation tradeoffs. The zeta parameter controls how smooth the estimates of the confidence are, lower values for smoother. (defaults to 0.01). Now, given estimates Q_1 and Q_2 for actions 1,2 respectively we assume a Laplacian distribution centered around Q_1 and Q_2, with variance equal to v_1 and v_2. With Gibbs sampling, the probability of selecting action Q_1 is then </p><p class="formulaDsp">
<img class="formulaDsp" alt="\[ P[1]=exp(Q_1/\sqrt{v_1})/(exp(Q_1/\sqrt{v_1})+exp(Q_2/\sqrt{v2})) \]" src="form_49.png"/>
</p>
<p>.</p>
<p>However it is possible to perform direct sampling.</p>
<p><a class="el" href="class_discrete_policy.html#a8e68a3cbd81386ac80f1e8ecc04591b2" title="Set forced learning (force-feed actions) ">setForcedLearning(bool forced)</a>; </p>

<p>Definition at line <a class="el" href="policy_8h_source.html#l00144">144</a> of file <a class="el" href="policy_8h_source.html">policy.h</a>.</p>
</div><h2 class="groupheader">Constructor &amp; Destructor Documentation</h2>
<a class="anchor" id="a38808c7e73befd06d12f631efd61046b"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">DiscretePolicy::DiscretePolicy </td>
          <td>(</td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>n_states</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>n_actions</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="real_8h.html#a031f8951175b43076c2084a6c2173410">real</a>&#160;</td>
          <td class="paramname"><em>alpha</em> = <code>0.1</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="real_8h.html#a031f8951175b43076c2084a6c2173410">real</a>&#160;</td>
          <td class="paramname"><em>gamma</em> = <code>0.8</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="real_8h.html#a031f8951175b43076c2084a6c2173410">real</a>&#160;</td>
          <td class="paramname"><em>lambda</em> = <code>0.8</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool&#160;</td>
          <td class="paramname"><em>softmax</em> = <code>false</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="real_8h.html#a031f8951175b43076c2084a6c2173410">real</a>&#160;</td>
          <td class="paramname"><em>randomness</em> = <code>0.1</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="real_8h.html#a031f8951175b43076c2084a6c2173410">real</a>&#160;</td>
          <td class="paramname"><em>init_eval</em> = <code>0.0</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Create a new discrete policy. </p>
<ul>
<li>n_states Number of states for the agent </li>
<li>n_actions Number of actions. </li>
<li>alpha Learning rate. </li>
<li>gamma Discount parameter. </li>
<li>lambda Eligibility trace decay. </li>
<li>softmax Use softmax if true (can be overridden later) </li>
<li>randomness Amount of randomness. </li>
<li>init_eval Initial evaluation of actions. </li>
</ul>

<p>Definition at line <a class="el" href="policy_8cpp_source.html#l00042">42</a> of file <a class="el" href="policy_8cpp_source.html">policy.cpp</a>.</p>

</div>
</div>
<a class="anchor" id="abcbcc865c18db7e34738b0c34e447208"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">DiscretePolicy::~DiscretePolicy </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Kill the agent and free everything. </p>
<p>Delete policy. </p>

<p>Definition at line <a class="el" href="policy_8cpp_source.html#l00155">155</a> of file <a class="el" href="policy_8cpp_source.html">policy.cpp</a>.</p>

<p><div class="dynheader">
Here is the call graph for this function:</div>
<div class="dyncontent">
<div class="center"><img src="class_discrete_policy_abcbcc865c18db7e34738b0c34e447208_cgraph.png" border="0" usemap="#class_discrete_policy_abcbcc865c18db7e34738b0c34e447208_cgraph" alt=""/></div>
<map name="class_discrete_policy_abcbcc865c18db7e34738b0c34e447208_cgraph" id="class_discrete_policy_abcbcc865c18db7e34738b0c34e447208_cgraph">
</map>
</div>
</p>

</div>
</div>
<h2 class="groupheader">Member Function Documentation</h2>
<a class="anchor" id="a280c85ce976de3150f772ed4c772c1e0"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">int DiscretePolicy::argMax </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="real_8h.html#a031f8951175b43076c2084a6c2173410">real</a> *&#160;</td>
          <td class="paramname"><em>Qs</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Get ID of maximum action. </p>

<p>Definition at line <a class="el" href="policy_8cpp_source.html#l00816">816</a> of file <a class="el" href="policy_8cpp_source.html">policy.cpp</a>.</p>

<p><div class="dynheader">
Here is the call graph for this function:</div>
<div class="dyncontent">
<div class="center"><img src="class_discrete_policy_a280c85ce976de3150f772ed4c772c1e0_cgraph.png" border="0" usemap="#class_discrete_policy_a280c85ce976de3150f772ed4c772c1e0_cgraph" alt=""/></div>
<map name="class_discrete_policy_a280c85ce976de3150f772ed4c772c1e0_cgraph" id="class_discrete_policy_a280c85ce976de3150f772ed4c772c1e0_cgraph">
</map>
</div>
</p>

</div>
</div>
<a class="anchor" id="aef97e522d2cd647a0260f0bc77508c05"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">int DiscretePolicy::confMax </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="real_8h.html#a031f8951175b43076c2084a6c2173410">real</a> *&#160;</td>
          <td class="paramname"><em>Qs</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="real_8h.html#a031f8951175b43076c2084a6c2173410">real</a> *&#160;</td>
          <td class="paramname"><em>vQs</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="real_8h.html#a031f8951175b43076c2084a6c2173410">real</a>&#160;</td>
          <td class="paramname"><em>p</em> = <code>1.0</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Confidence-based Gibbs sampling. </p>

<p>Definition at line <a class="el" href="policy_8cpp_source.html#l00715">715</a> of file <a class="el" href="policy_8cpp_source.html">policy.cpp</a>.</p>

<p><div class="dynheader">
Here is the call graph for this function:</div>
<div class="dyncontent">
<div class="center"><img src="class_discrete_policy_aef97e522d2cd647a0260f0bc77508c05_cgraph.png" border="0" usemap="#class_discrete_policy_aef97e522d2cd647a0260f0bc77508c05_cgraph" alt=""/></div>
<map name="class_discrete_policy_aef97e522d2cd647a0260f0bc77508c05_cgraph" id="class_discrete_policy_aef97e522d2cd647a0260f0bc77508c05_cgraph">
</map>
</div>
</p>

</div>
</div>
<a class="anchor" id="a0d68a28991722e21bf90f7f34fa7e41e"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">int DiscretePolicy::confSample </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="real_8h.html#a031f8951175b43076c2084a6c2173410">real</a> *&#160;</td>
          <td class="paramname"><em>Qs</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="real_8h.html#a031f8951175b43076c2084a6c2173410">real</a> *&#160;</td>
          <td class="paramname"><em>vQs</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Directly sample from action value distribution. </p>

<p>Definition at line <a class="el" href="policy_8cpp_source.html#l00749">749</a> of file <a class="el" href="policy_8cpp_source.html">policy.cpp</a>.</p>

<p><div class="dynheader">
Here is the call graph for this function:</div>
<div class="dyncontent">
<div class="center"><img src="class_discrete_policy_a0d68a28991722e21bf90f7f34fa7e41e_cgraph.png" border="0" usemap="#class_discrete_policy_a0d68a28991722e21bf90f7f34fa7e41e_cgraph" alt=""/></div>
<map name="class_discrete_policy_a0d68a28991722e21bf90f7f34fa7e41e_cgraph" id="class_discrete_policy_a0d68a28991722e21bf90f7f34fa7e41e_cgraph">
</map>
</div>
</p>

</div>
</div>
<a class="anchor" id="a969a17dda8b4a094f0f9ea0076aefa5b"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">int DiscretePolicy::eGreedy </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="real_8h.html#a031f8951175b43076c2084a6c2173410">real</a> *&#160;</td>
          <td class="paramname"><em>Qs</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>e-greedy sampling </p>

<p>Definition at line <a class="el" href="policy_8cpp_source.html#l00802">802</a> of file <a class="el" href="policy_8cpp_source.html">policy.cpp</a>.</p>

<p><div class="dynheader">
Here is the call graph for this function:</div>
<div class="dyncontent">
<div class="center"><img src="class_discrete_policy_a969a17dda8b4a094f0f9ea0076aefa5b_cgraph.png" border="0" usemap="#class_discrete_policy_a969a17dda8b4a094f0f9ea0076aefa5b_cgraph" alt=""/></div>
<map name="class_discrete_policy_a969a17dda8b4a094f0f9ea0076aefa5b_cgraph" id="class_discrete_policy_a969a17dda8b4a094f0f9ea0076aefa5b_cgraph">
</map>
</div>
</p>

</div>
</div>
<a class="anchor" id="a7c51bd2e37d6906d003e46e5af1f0c54"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">virtual <a class="el" href="real_8h.html#a031f8951175b43076c2084a6c2173410">real</a> DiscretePolicy::getLastActionValue </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Get the vale of the last action taken. </p>

<p>Reimplemented in <a class="el" href="class_a_n_n___policy.html#a1ece8c97fe6220bd2b5ee3da06a7751c">ANN_Policy</a>.</p>

<p>Definition at line <a class="el" href="policy_8h_source.html#l00195">195</a> of file <a class="el" href="policy_8h_source.html">policy.h</a>.</p>

<p><div class="dynheader">
Here is the call graph for this function:</div>
<div class="dyncontent">
<div class="center"><img src="class_discrete_policy_a7c51bd2e37d6906d003e46e5af1f0c54_cgraph.png" border="0" usemap="#class_discrete_policy_a7c51bd2e37d6906d003e46e5af1f0c54_cgraph" alt=""/></div>
<map name="class_discrete_policy_a7c51bd2e37d6906d003e46e5af1f0c54_cgraph" id="class_discrete_policy_a7c51bd2e37d6906d003e46e5af1f0c54_cgraph">
</map>
</div>
</p>

</div>
</div>
<a class="anchor" id="a95f9342465b2881cc39fbb51427480d0"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">virtual <a class="el" href="real_8h.html#a031f8951175b43076c2084a6c2173410">real</a> DiscretePolicy::getTDError </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Get the temporal difference error of the <em>previous</em> action. </p>

<p>Definition at line <a class="el" href="policy_8h_source.html#l00193">193</a> of file <a class="el" href="policy_8h_source.html">policy.h</a>.</p>

</div>
</div>
<a class="anchor" id="a6058b276038c93031bf2020652e7fe80"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void DiscretePolicy::loadFile </td>
          <td>(</td>
          <td class="paramtype">char *&#160;</td>
          <td class="paramname"><em>f</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Load policy from a file. </p>

<p>Definition at line <a class="el" href="policy_8cpp_source.html#l00484">484</a> of file <a class="el" href="policy_8cpp_source.html">policy.cpp</a>.</p>

<p><div class="dynheader">
Here is the call graph for this function:</div>
<div class="dyncontent">
<div class="center"><img src="class_discrete_policy_a6058b276038c93031bf2020652e7fe80_cgraph.png" border="0" usemap="#class_discrete_policy_a6058b276038c93031bf2020652e7fe80_cgraph" alt=""/></div>
<map name="class_discrete_policy_a6058b276038c93031bf2020652e7fe80_cgraph" id="class_discrete_policy_a6058b276038c93031bf2020652e7fe80_cgraph">
</map>
</div>
</p>

</div>
</div>
<a class="anchor" id="a85f150551fe5ffafe5d92edfa4f52099"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void DiscretePolicy::Reset </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Use at the end of every episode, after agent has entered the absorbing state. </p>

<p>Reimplemented in <a class="el" href="class_a_n_n___policy.html#a76adab5a1588ae2e52d80a7ce2b782ac">ANN_Policy</a>.</p>

<p>Definition at line <a class="el" href="policy_8cpp_source.html#l00474">474</a> of file <a class="el" href="policy_8cpp_source.html">policy.cpp</a>.</p>

</div>
</div>
<a class="anchor" id="a44500c0d11b1866f4aff8e291f1ed68b"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void DiscretePolicy::saveFile </td>
          <td>(</td>
          <td class="paramtype">char *&#160;</td>
          <td class="paramname"><em>f</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Save policy to a file. </p>

<p>Definition at line <a class="el" href="policy_8cpp_source.html#l00550">550</a> of file <a class="el" href="policy_8cpp_source.html">policy.cpp</a>.</p>

</div>
</div>
<a class="anchor" id="a52843d1894e52680a66ed95540620ebe"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void DiscretePolicy::saveState </td>
          <td>(</td>
          <td class="paramtype">FILE *&#160;</td>
          <td class="paramname"><em>f</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Save the current evaluations in text format to a file. </p>
<p>The values are saved as triplets (<code>Q</code>, <code>P</code>, <code>vQ</code>). The columns are ordered by actions and the rows by state number. </p>

<p>Definition at line <a class="el" href="policy_8cpp_source.html#l00128">128</a> of file <a class="el" href="policy_8cpp_source.html">policy.cpp</a>.</p>

</div>
</div>
<a class="anchor" id="a5783d913368870603cb8adbeeee30923"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">int DiscretePolicy::SelectAction </td>
          <td>(</td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>s</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="real_8h.html#a031f8951175b43076c2084a6c2173410">real</a>&#160;</td>
          <td class="paramname"><em>r</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>forced_a</em> = <code>-1</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Select an action a, given state s and reward from previous action. </p>
<p>Optional argument a forces an action if <a class="el" href="class_discrete_policy.html#a8e68a3cbd81386ac80f1e8ecc04591b2" title="Set forced learning (force-feed actions) ">setForcedLearning()</a> has been called with true.</p>
<p>Two algorithms are implemented, both of which converge. One of them calculates the value of the current policy, while the other that of the optimal policy.</p>
<p>Sarsa ( <img class="formulaInl" alt="$\lambda$" src="form_24.png"/>) algorithmic description:</p>
<ol type="1">
<li>Take action <img class="formulaInl" alt="$a$" src="form_25.png"/>, observe <img class="formulaInl" alt="$r, s'$" src="form_26.png"/></li>
<li>Choose <img class="formulaInl" alt="$a'$" src="form_27.png"/> from <img class="formulaInl" alt="$s'$" src="form_28.png"/> using some policy derived from <img class="formulaInl" alt="$Q$" src="form_29.png"/></li>
<li><img class="formulaInl" alt="$\delta = r + \gamma Q(s',a') - Q(s,a)$" src="form_30.png"/></li>
<li><img class="formulaInl" alt="$e(s,a) = e(s,a)+ 1$" src="form_31.png"/>, depending on trace settings</li>
<li>for all <img class="formulaInl" alt="$s,a$" src="form_32.png"/> : <p class="formulaDsp">
<img class="formulaDsp" alt="\[ Q_{t}(s,a) = Q_{t-1}(s,a) + \alpha \delta e_{t}(s,a), \]" src="form_33.png"/>
</p>
 where <img class="formulaInl" alt="$e_{t}(s,a) = \gamma \lambda e_{t-1}(s,a)$" src="form_34.png"/> <pre class="fragment">  end
</pre></li>
<li><img class="formulaInl" alt="$a = a'$" src="form_35.png"/> (we will take this action at the next step)</li>
<li><img class="formulaInl" alt="$s = s'$" src="form_36.png"/></li>
</ol>
<p>Watkins Q (l) algorithmic description:</p>
<ol type="1">
<li>Take action <img class="formulaInl" alt="$a$" src="form_25.png"/>, observe <img class="formulaInl" alt="$r$" src="form_37.png"/>, <img class="formulaInl" alt="$s'$" src="form_28.png"/></li>
<li>Choose <img class="formulaInl" alt="$a'$" src="form_27.png"/> from <img class="formulaInl" alt="$s'$" src="form_28.png"/> using some policy derived from <img class="formulaInl" alt="$Q$" src="form_29.png"/></li>
<li><img class="formulaInl" alt="$a* = \arg \max_b Q(s',b)$" src="form_38.png"/></li>
</ol>
<ol type="1">
<li><img class="formulaInl" alt="$\delta = r + \gamma Q(s',a^*) - Q(s,a)$" src="form_39.png"/></li>
<li><img class="formulaInl" alt="$e(s,a) = e(s,a)+ 1$" src="form_31.png"/>, depending on eligibility traces</li>
<li>for all <img class="formulaInl" alt="$s,a$" src="form_32.png"/> : <p class="formulaDsp">
<img class="formulaDsp" alt="\[ Q(s,a) = Q(s,a)+\alpha \delta e(s,a) \]" src="form_40.png"/>
</p>
 if <img class="formulaInl" alt="$(a'=a*)$" src="form_41.png"/> then <img class="formulaInl" alt="$e(s,a)$" src="form_42.png"/> = <img class="formulaInl" alt="$\gamma \lambda e(s,a)$" src="form_43.png"/> else <img class="formulaInl" alt="$e(s,a) = 0$" src="form_44.png"/> end</li>
<li><img class="formulaInl" alt="$a = a'$" src="form_35.png"/> (we will take this action at the next step)</li>
<li><img class="formulaInl" alt="$s = s'$" src="form_36.png"/></li>
</ol>
<p>The most general algorithm is E-learning, currently under development, which is defined as follows:</p>
<ol type="1">
<li>Take action <img class="formulaInl" alt="$a$" src="form_25.png"/>, observe <img class="formulaInl" alt="$r$" src="form_37.png"/>, <img class="formulaInl" alt="$s'$" src="form_28.png"/></li>
<li>Choose <img class="formulaInl" alt="$a'$" src="form_27.png"/> from <img class="formulaInl" alt="$s'$" src="form_28.png"/> using some policy derived from <img class="formulaInl" alt="$Q$" src="form_29.png"/></li>
<li><img class="formulaInl" alt="$\delta = r + \gamma E{Q(s',a^*)|\pi} - Q(s,a)$" src="form_45.png"/></li>
<li><img class="formulaInl" alt="$e(s,a) = e(s,a)+ 1$" src="form_31.png"/>, depending on eligibility traces</li>
<li>for all <img class="formulaInl" alt="$s,a$" src="form_32.png"/> : <p class="formulaDsp">
<img class="formulaDsp" alt="\[ Q(s,a) = Q(s,a)+\alpha \delta e(s,a) \]" src="form_40.png"/>
</p>
 <img class="formulaInl" alt="$e(s,a)$" src="form_42.png"/> = <img class="formulaInl" alt="$\gamma \lambda e(s,a) P(a|s,\pi) $" src="form_46.png"/></li>
<li><img class="formulaInl" alt="$a = a'$" src="form_35.png"/> (we will take this action at the next step)</li>
<li><img class="formulaInl" alt="$s = s'$" src="form_36.png"/></li>
</ol>
<p>Note that we also cut off the eligibility traces that have fallen below 0.1 </p>

<p>Definition at line <a class="el" href="policy_8cpp_source.html#l00283">283</a> of file <a class="el" href="policy_8cpp_source.html">policy.cpp</a>.</p>

<p><div class="dynheader">
Here is the call graph for this function:</div>
<div class="dyncontent">
<div class="center"><img src="class_discrete_policy_a5783d913368870603cb8adbeeee30923_cgraph.png" border="0" usemap="#class_discrete_policy_a5783d913368870603cb8adbeeee30923_cgraph" alt=""/></div>
<map name="class_discrete_policy_a5783d913368870603cb8adbeeee30923_cgraph" id="class_discrete_policy_a5783d913368870603cb8adbeeee30923_cgraph">
</map>
</div>
</p>

</div>
</div>
<a class="anchor" id="a9cf86ac98c427cf3e8433d423c934b38"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void DiscretePolicy::setConfidenceDistribution </td>
          <td>(</td>
          <td class="paramtype">enum <a class="el" href="policy_8h.html#a9517ab9def7b708d35806750d111ed8a">ConfidenceDistribution</a>&#160;</td>
          <td class="paramname"><em>cd</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Set the distribution for direct action sampling. </p>

<p>Definition at line <a class="el" href="policy_8cpp_source.html#l00684">684</a> of file <a class="el" href="policy_8cpp_source.html">policy.cpp</a>.</p>

</div>
</div>
<a class="anchor" id="ae1947bf5cb1ea7ce5d5b8a826406de8a"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void DiscretePolicy::setELearning </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Set the algorithm to ELearning mode. </p>

<p>Definition at line <a class="el" href="policy_8cpp_source.html#l00604">604</a> of file <a class="el" href="policy_8cpp_source.html">policy.cpp</a>.</p>

</div>
</div>
<a class="anchor" id="a8e68a3cbd81386ac80f1e8ecc04591b2"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void DiscretePolicy::setForcedLearning </td>
          <td>(</td>
          <td class="paramtype">bool&#160;</td>
          <td class="paramname"><em>forced</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Set forced learning (force-feed actions) </p>

<p>Definition at line <a class="el" href="policy_8cpp_source.html#l00639">639</a> of file <a class="el" href="policy_8cpp_source.html">policy.cpp</a>.</p>

</div>
</div>
<a class="anchor" id="a558dc462679a258d6dd8645af9774eaf"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void DiscretePolicy::setGamma </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="real_8h.html#a031f8951175b43076c2084a6c2173410">real</a>&#160;</td>
          <td class="paramname"><em>gamma</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Set the gamma of the sum to be maximised. </p>

<p>Definition at line <a class="el" href="policy_8cpp_source.html#l00656">656</a> of file <a class="el" href="policy_8cpp_source.html">policy.cpp</a>.</p>

</div>
</div>
<a class="anchor" id="ad2ce661316086f128b56a43031886f85"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">virtual void DiscretePolicy::setLearningRate </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="real_8h.html#a031f8951175b43076c2084a6c2173410">real</a>&#160;</td>
          <td class="paramname"><em>alpha</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Set the learning rate. </p>

<p>Definition at line <a class="el" href="policy_8h_source.html#l00191">191</a> of file <a class="el" href="policy_8h_source.html">policy.h</a>.</p>

</div>
</div>
<a class="anchor" id="a0d4b562ff61ed37228bdeca1c4d89056"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void DiscretePolicy::setPursuit </td>
          <td>(</td>
          <td class="paramtype">bool&#160;</td>
          <td class="paramname"><em>pursuit</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Use Pursuit for action selection. </p>

<p>Definition at line <a class="el" href="policy_8cpp_source.html#l00618">618</a> of file <a class="el" href="policy_8cpp_source.html">policy.cpp</a>.</p>

</div>
</div>
<a class="anchor" id="a32efaeac5af1bf8b5a74a9ef4644d858"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void DiscretePolicy::setQLearning </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Set the algorithm to QLearning mode. </p>

<p>Definition at line <a class="el" href="policy_8cpp_source.html#l00598">598</a> of file <a class="el" href="policy_8cpp_source.html">policy.cpp</a>.</p>

</div>
</div>
<a class="anchor" id="a369c11fbf923820089a6d2cdfd2ef8fe"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void DiscretePolicy::setRandomness </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="real_8h.html#a031f8951175b43076c2084a6c2173410">real</a>&#160;</td>
          <td class="paramname"><em>epsilon</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Set randomness for action selection. Does not affect confidence mode. </p>

<p>Definition at line <a class="el" href="policy_8cpp_source.html#l00645">645</a> of file <a class="el" href="policy_8cpp_source.html">policy.cpp</a>.</p>

</div>
</div>
<a class="anchor" id="a9486c0fd9c2d51954eb048597d681495"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void DiscretePolicy::setReplacingTraces </td>
          <td>(</td>
          <td class="paramtype">bool&#160;</td>
          <td class="paramname"><em>replacing</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Use Pursuit for action selection. </p>

<p>Definition at line <a class="el" href="policy_8cpp_source.html#l00629">629</a> of file <a class="el" href="policy_8cpp_source.html">policy.cpp</a>.</p>

</div>
</div>
<a class="anchor" id="a99195ad7301204d62186703e489d687f"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void DiscretePolicy::setSarsa </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Set the algorithm to SARSA mode. </p>
<p>A unified framework for action selection. </p>

<p>Definition at line <a class="el" href="policy_8cpp_source.html#l00611">611</a> of file <a class="el" href="policy_8cpp_source.html">policy.cpp</a>.</p>

</div>
</div>
<a class="anchor" id="a1a82bb1b411851f8baeedd532b4a4bc1"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">int DiscretePolicy::softMax </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="real_8h.html#a031f8951175b43076c2084a6c2173410">real</a> *&#160;</td>
          <td class="paramname"><em>Qs</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Softmax Gibbs sampling. </p>

<p>Definition at line <a class="el" href="policy_8cpp_source.html#l00783">783</a> of file <a class="el" href="policy_8cpp_source.html">policy.cpp</a>.</p>

<p><div class="dynheader">
Here is the call graph for this function:</div>
<div class="dyncontent">
<div class="center"><img src="class_discrete_policy_a1a82bb1b411851f8baeedd532b4a4bc1_cgraph.png" border="0" usemap="#class_discrete_policy_a1a82bb1b411851f8baeedd532b4a4bc1_cgraph" alt=""/></div>
<map name="class_discrete_policy_a1a82bb1b411851f8baeedd532b4a4bc1_cgraph" id="class_discrete_policy_a1a82bb1b411851f8baeedd532b4a4bc1_cgraph">
</map>
</div>
</p>

</div>
</div>
<a class="anchor" id="aa9566ce8fc4be68c883e9c067bdb6bc3"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">bool DiscretePolicy::useConfidenceEstimates </td>
          <td>(</td>
          <td class="paramtype">bool&#160;</td>
          <td class="paramname"><em>confidence</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="real_8h.html#a031f8951175b43076c2084a6c2173410">real</a>&#160;</td>
          <td class="paramname"><em>zeta</em> = <code>0.01</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool&#160;</td>
          <td class="paramname"><em>confidence_eligibility</em> = <code>false</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Set to use confidence estimates for action selection, with variance smoothing zeta. </p>
<p>Variance smoothing currently uses a very simple method to estimate the variance. </p>

<p>Definition at line <a class="el" href="policy_8cpp_source.html#l00580">580</a> of file <a class="el" href="policy_8cpp_source.html">policy.cpp</a>.</p>

</div>
</div>
<a class="anchor" id="aead952ca5598727cbff910796aca9865"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void DiscretePolicy::useGibbsConfidence </td>
          <td>(</td>
          <td class="paramtype">bool&#160;</td>
          <td class="paramname"><em>gibbs</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Add Gibbs sampling for confidences. </p>
<p>This can be used in conjuction with any confidence distribution, however it mostly makes sense for SINGULAR. </p>

<p>Definition at line <a class="el" href="policy_8cpp_source.html#l00704">704</a> of file <a class="el" href="policy_8cpp_source.html">policy.cpp</a>.</p>

</div>
</div>
<a class="anchor" id="ad8bc316900503b360bae7a19ce714a12"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void DiscretePolicy::useReliabilityEstimate </td>
          <td>(</td>
          <td class="paramtype">bool&#160;</td>
          <td class="paramname"><em>ri</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Use the reliability estimate method for action selection. </p>

<p>Definition at line <a class="el" href="policy_8cpp_source.html#l00673">673</a> of file <a class="el" href="policy_8cpp_source.html">policy.cpp</a>.</p>

</div>
</div>
<a class="anchor" id="a676e1f828240b838e8c76ba9bd9183c0"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void DiscretePolicy::useSoftmax </td>
          <td>(</td>
          <td class="paramtype">bool&#160;</td>
          <td class="paramname"><em>softmax</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Set action selection to softmax. </p>

<p>Definition at line <a class="el" href="policy_8cpp_source.html#l00662">662</a> of file <a class="el" href="policy_8cpp_source.html">policy.cpp</a>.</p>

</div>
</div>
<h2 class="groupheader">Member Data Documentation</h2>
<a class="anchor" id="aa2532317171fce765c11c554811821ba"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="real_8h.html#a031f8951175b43076c2084a6c2173410">real</a> DiscretePolicy::alpha</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>learning rate </p>

<p>Definition at line <a class="el" href="policy_8h_source.html#l00166">166</a> of file <a class="el" href="policy_8h_source.html">policy.h</a>.</p>

</div>
</div>
<a class="anchor" id="aa09cc655f3963a9599a4931e94fd1ad5"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">bool DiscretePolicy::confidence</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Confidence estimates option. </p>

<p>Definition at line <a class="el" href="policy_8h_source.html#l00174">174</a> of file <a class="el" href="policy_8h_source.html">policy.h</a>.</p>

</div>
</div>
<a class="anchor" id="ad2d84a7ca4c2a8c8a807a11f73041df7"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">enum <a class="el" href="policy_8h.html#a9517ab9def7b708d35806750d111ed8a">ConfidenceDistribution</a> DiscretePolicy::confidence_distribution</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p><a class="el" href="class_distribution.html" title="Probability distribution. ">Distribution</a> to use for confidence sampling. </p>

<p>Definition at line <a class="el" href="policy_8h_source.html#l00177">177</a> of file <a class="el" href="policy_8h_source.html">policy.h</a>.</p>

</div>
</div>
<a class="anchor" id="ad147be13ff89ba39072a870dd34b2be7"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">bool DiscretePolicy::confidence_eligibility</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Apply eligibility traces to confidence. </p>

<p>Definition at line <a class="el" href="policy_8h_source.html#l00175">175</a> of file <a class="el" href="policy_8h_source.html">policy.h</a>.</p>

</div>
</div>
<a class="anchor" id="ab4674e239def6c36c8b814ca933e2c95"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">bool DiscretePolicy::confidence_uses_gibbs</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Additional gibbs sampling for confidence. </p>

<p>Definition at line <a class="el" href="policy_8h_source.html#l00178">178</a> of file <a class="el" href="policy_8h_source.html">policy.h</a>.</p>

</div>
</div>
<a class="anchor" id="a967c043721521692685f722cd19dde0c"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="real_8h.html#a031f8951175b43076c2084a6c2173410">real</a>** DiscretePolicy::e</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>eligibility trace </p>

<p>Definition at line <a class="el" href="policy_8h_source.html#l00152">152</a> of file <a class="el" href="policy_8h_source.html">policy.h</a>.</p>

</div>
</div>
<a class="anchor" id="af8e4af6ae154997cc62107c2881c743f"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="real_8h.html#a031f8951175b43076c2084a6c2173410">real</a>* DiscretePolicy::eval</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>evaluation of current aciton </p>

<p>Definition at line <a class="el" href="policy_8h_source.html#l00153">153</a> of file <a class="el" href="policy_8h_source.html">policy.h</a>.</p>

</div>
</div>
<a class="anchor" id="a7dcd73587c35dcc0ee4c88a90a4934d5"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="real_8h.html#a031f8951175b43076c2084a6c2173410">real</a> DiscretePolicy::expected_r</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Expected reward. </p>

<p>Definition at line <a class="el" href="policy_8h_source.html#l00167">167</a> of file <a class="el" href="policy_8h_source.html">policy.h</a>.</p>

</div>
</div>
<a class="anchor" id="ac971a0b4531befd7e35b0d213abfc18b"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="real_8h.html#a031f8951175b43076c2084a6c2173410">real</a> DiscretePolicy::expected_V</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Expected state return. </p>

<p>Definition at line <a class="el" href="policy_8h_source.html#l00168">168</a> of file <a class="el" href="policy_8h_source.html">policy.h</a>.</p>

</div>
</div>
<a class="anchor" id="a8612bd729565c78d029fe321981bc640"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">bool DiscretePolicy::forced_learning</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Force agent to take supplied action. </p>

<p>Definition at line <a class="el" href="policy_8h_source.html#l00173">173</a> of file <a class="el" href="policy_8h_source.html">policy.h</a>.</p>

</div>
</div>
<a class="anchor" id="abf219f066c64c027d8340e5f1f864117"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="real_8h.html#a031f8951175b43076c2084a6c2173410">real</a> DiscretePolicy::gamma</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Future discount parameter. </p>

<p>Definition at line <a class="el" href="policy_8h_source.html#l00164">164</a> of file <a class="el" href="policy_8h_source.html">policy.h</a>.</p>

</div>
</div>
<a class="anchor" id="a937f55bda1533e42c0198965ccb7e7b8"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="real_8h.html#a031f8951175b43076c2084a6c2173410">real</a> DiscretePolicy::lambda</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Eligibility trace decay. </p>

<p>Definition at line <a class="el" href="policy_8h_source.html#l00165">165</a> of file <a class="el" href="policy_8h_source.html">policy.h</a>.</p>

</div>
</div>
<a class="anchor" id="a6a6aef0d063dde7ae1037effe7d3d9e4"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">enum <a class="el" href="policy_8h.html#aff580e7ad2896c3a6b8863811234469a">LearningMethod</a> DiscretePolicy::learning_method</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>learning method to use; </p>

<p>Definition at line <a class="el" href="policy_8h_source.html#l00148">148</a> of file <a class="el" href="policy_8h_source.html">policy.h</a>.</p>

</div>
</div>
<a class="anchor" id="af73d4c7ea1251b74c3b6ee1f112d7308"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">int DiscretePolicy::max_el_state</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>max state ID to search for eligibility </p>

<p>Definition at line <a class="el" href="policy_8h_source.html#l00171">171</a> of file <a class="el" href="policy_8h_source.html">policy.h</a>.</p>

</div>
</div>
<a class="anchor" id="a91a4b8869983fc306f666c8a53e3c392"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">int DiscretePolicy::min_el_state</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>min state ID to search for eligibility </p>

<p>Definition at line <a class="el" href="policy_8h_source.html#l00170">170</a> of file <a class="el" href="policy_8h_source.html">policy.h</a>.</p>

</div>
</div>
<a class="anchor" id="a4ce76a43e2c48cd148f1de16ffc302ab"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">int DiscretePolicy::n_actions</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>number of actions </p>

<p>Definition at line <a class="el" href="policy_8h_source.html#l00150">150</a> of file <a class="el" href="policy_8h_source.html">policy.h</a>.</p>

</div>
</div>
<a class="anchor" id="af4e2f051947d822f48ad41e72d9ace60"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">int DiscretePolicy::n_samples</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>number of samples for above expected r and V </p>

<p>Definition at line <a class="el" href="policy_8h_source.html#l00169">169</a> of file <a class="el" href="policy_8h_source.html">policy.h</a>.</p>

</div>
</div>
<a class="anchor" id="a0299d2d2977c70d5b95e8f5e22564ad9"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">int DiscretePolicy::n_states</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>number of states </p>

<p>Definition at line <a class="el" href="policy_8h_source.html#l00149">149</a> of file <a class="el" href="policy_8h_source.html">policy.h</a>.</p>

</div>
</div>
<a class="anchor" id="a321300d54340917a8c18aa5bdf70e957"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="real_8h.html#a031f8951175b43076c2084a6c2173410">real</a>** DiscretePolicy::P</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>pursuit action probabilities </p>

<p>Definition at line <a class="el" href="policy_8h_source.html#l00163">163</a> of file <a class="el" href="policy_8h_source.html">policy.h</a>.</p>

</div>
</div>
<a class="anchor" id="af9c9c25ebbc6099a94b7272d97ef7cd1"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">int DiscretePolicy::pa</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>previous action </p>

<p>Definition at line <a class="el" href="policy_8h_source.html#l00157">157</a> of file <a class="el" href="policy_8h_source.html">policy.h</a>.</p>

</div>
</div>
<a class="anchor" id="af732e32c30602639a8a413bad23e7a68"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="real_8h.html#a031f8951175b43076c2084a6c2173410">real</a> DiscretePolicy::pQ</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>previous Q </p>

<p>Definition at line <a class="el" href="policy_8h_source.html#l00155">155</a> of file <a class="el" href="policy_8h_source.html">policy.h</a>.</p>

</div>
</div>
<a class="anchor" id="ac5f57533c6235ea465790dbc0ca67d9b"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">int DiscretePolicy::ps</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>previous state </p>

<p>Definition at line <a class="el" href="policy_8h_source.html#l00156">156</a> of file <a class="el" href="policy_8h_source.html">policy.h</a>.</p>

</div>
</div>
<a class="anchor" id="aa4b64c80c15b12a77e8aeeb0af677254"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">bool DiscretePolicy::pursuit</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>pursuit option </p>

<p>Definition at line <a class="el" href="policy_8h_source.html#l00162">162</a> of file <a class="el" href="policy_8h_source.html">policy.h</a>.</p>

</div>
</div>
<a class="anchor" id="ae938f9b15ca6dc7ee6b81f17911a3985"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="real_8h.html#a031f8951175b43076c2084a6c2173410">real</a>** DiscretePolicy::Q</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>state-action evaluation </p>

<p>Definition at line <a class="el" href="policy_8h_source.html#l00151">151</a> of file <a class="el" href="policy_8h_source.html">policy.h</a>.</p>

</div>
</div>
<a class="anchor" id="aba429d4060d6b1f72cc3ce603bff4f0d"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="real_8h.html#a031f8951175b43076c2084a6c2173410">real</a> DiscretePolicy::r</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>reward </p>

<p>Definition at line <a class="el" href="policy_8h_source.html#l00158">158</a> of file <a class="el" href="policy_8h_source.html">policy.h</a>.</p>

</div>
</div>
<a class="anchor" id="a9202989ffb7177a4c9145a01191abbae"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">bool DiscretePolicy::reliability_estimate</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>reliability estimates option </p>

<p>Definition at line <a class="el" href="policy_8h_source.html#l00176">176</a> of file <a class="el" href="policy_8h_source.html">policy.h</a>.</p>

</div>
</div>
<a class="anchor" id="a39c6a2209b93281890fdf2979eb80e07"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">bool DiscretePolicy::replacing_traces</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Replacing instead of accumulating traces. </p>

<p>Definition at line <a class="el" href="policy_8h_source.html#l00172">172</a> of file <a class="el" href="policy_8h_source.html">policy.h</a>.</p>

</div>
</div>
<a class="anchor" id="a87dafb916b43cbdb7afada5123be0b57"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="real_8h.html#a031f8951175b43076c2084a6c2173410">real</a>* DiscretePolicy::sample</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>sampling output </p>

<p>Definition at line <a class="el" href="policy_8h_source.html#l00154">154</a> of file <a class="el" href="policy_8h_source.html">policy.h</a>.</p>

</div>
</div>
<a class="anchor" id="aa7dda786a5b43c89a1430f6d08cc04c2"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">bool DiscretePolicy::smax</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>softmax option </p>

<p>Definition at line <a class="el" href="policy_8h_source.html#l00161">161</a> of file <a class="el" href="policy_8h_source.html">policy.h</a>.</p>

</div>
</div>
<a class="anchor" id="a2a324879a3aa500791c42eb844c00623"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="real_8h.html#a031f8951175b43076c2084a6c2173410">real</a> DiscretePolicy::tdError</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>temporal difference error </p>

<p>Definition at line <a class="el" href="policy_8h_source.html#l00160">160</a> of file <a class="el" href="policy_8h_source.html">policy.h</a>.</p>

</div>
</div>
<a class="anchor" id="ab03ecbc5cc0c50264069f17ce8ad7430"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="real_8h.html#a031f8951175b43076c2084a6c2173410">real</a> DiscretePolicy::temp</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>scratch </p>

<p>Definition at line <a class="el" href="policy_8h_source.html#l00159">159</a> of file <a class="el" href="policy_8h_source.html">policy.h</a>.</p>

</div>
</div>
<a class="anchor" id="a0fc9cc4ca7542cfbce9363b62d243ff6"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="real_8h.html#a031f8951175b43076c2084a6c2173410">real</a>** DiscretePolicy::vQ</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>variance estimate for Q </p>

<p>Definition at line <a class="el" href="policy_8h_source.html#l00180">180</a> of file <a class="el" href="policy_8h_source.html">policy.h</a>.</p>

</div>
</div>
<a class="anchor" id="a66ff64c205debffe957ca8c904dfcecd"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="real_8h.html#a031f8951175b43076c2084a6c2173410">real</a> DiscretePolicy::zeta</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Confidence smoothing. </p>

<p>Definition at line <a class="el" href="policy_8h_source.html#l00179">179</a> of file <a class="el" href="policy_8h_source.html">policy.h</a>.</p>

</div>
</div>
<hr/>The documentation for this class was generated from the following files:<ul>
<li>src/libs/learning/<a class="el" href="policy_8h_source.html">policy.h</a></li>
<li>src/libs/learning/<a class="el" href="policy_8cpp_source.html">policy.cpp</a></li>
</ul>
</div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="class_discrete_policy.html">DiscretePolicy</a></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.11 </li>
  </ul>
</div>
</body>
</html>
